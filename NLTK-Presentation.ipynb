{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b05589f0-bc9f-4d3a-9b81-06ad5c99702b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python's Natural Language Toolkit - NLTK\n",
    " ##### Keziah Sheldon, Eesha Das Gupta, Rachel Buttry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "abbbf0bb-9f2d-443a-a519-8a6471d0795f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Natural Language Processing?\n",
    "\n",
    "\"Natural language processing is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages.\" -[Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "\n",
    "Natural Language Processing is the way for computers to process the human spoken and written language for analysis.\n",
    "\n",
    "Link: ['How Has the MFA Changed the Contemporary Novel?](http://www.theatlantic.com/entertainment/archive/2016/03/mfa-creative-writing/462483/) \n",
    "\n",
    "Link: [Wisdom of Chopra](http://wisdomofchopra.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c978d3f9-93c1-45ff-975b-ac796a1422ba"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Natural Language ToolKit](http://www.nltk.org/)\n",
    "\n",
    "* Python module for NLP\n",
    "* Provides interfaces to over 50 corpora and lexical resources\n",
    "* Tools for tokenization, stemming, tagging, parsing, andd semantic reasoning\n",
    "* Wrappers for industrial-strength NLP libraries\n",
    "* An intuitive and streamlined way to using NLP with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Example NLP Pipeline for NLTK\n",
    "\n",
    "![title](./pipeline1.png)\n",
    "\n",
    "[Image source](http://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "62085755-f394-422d-a675-4e8cdf6b1469"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizing\n",
    "\n",
    "* Tokenization is the first step of NLP (Natural Language Processing)\n",
    "* Split strings into substrings\n",
    "* Separate into sentences, identifies where they start and stop\n",
    "\n",
    "### For example, the Punkt tokenizer: (from docstring)\n",
    "This tokenizer divides a text into a list of sentences,\n",
    "by using an unsupervised algorithm to build a model for abbreviation\n",
    "words, collocations, and words that start sentences.  It must be\n",
    "trained on a large collection of plaintext in the target language\n",
    "before it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenize by sentence:\n",
    "* Doesn’t just depend on punctuation since recent decades have much more ambiguous punctuation usage (emoticons, abbreviations etc.)\n",
    "* Depends on the pre-trained 'tokenizer', e.g, Punkt, Stanford, Regex etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is PHYST480 Big Data class.', 'Natural language processing with the Natural Language Tool Kit module.', 'Much machine learning.', 'So wow.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"This is PHYST480 Big Data class. Natural language processing with the Natural Language Tool Kit module. Much machine learning. So wow.\"\n",
    "\n",
    "sent_tokenize_list = nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "print sent_tokenize_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fabb1615-376b-4b18-90e4-bfdb4fce8bda"
    }
   },
   "source": [
    "## Tokenize by word\n",
    "* Splits string by word\n",
    "* Punkt tokenizer also has word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.']\n"
     ]
    }
   ],
   "source": [
    "print nltk.tokenize.word_tokenize('Hello World.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78cd1ee7-a50f-412e-adf5-6035341a4c69"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "\n",
    "* POS is grammatical tagging\n",
    "* Separates into nouns, adjectives, verbs etc.\n",
    "* Can use pre-trained taggers or train yourself\n",
    "\n",
    "Types of POS tags:\n",
    "* JJ: adjective or numeral, ordinal\n",
    "* NNP: noun, proper, singular\n",
    "* IN: preposition or conjunction, subordinating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('cake', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('lie', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = nltk.word_tokenize('The cake is a lie.')\n",
    "\n",
    "print nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92f780f0-b790-4b26-87bb-d934e3c7ef82"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the the process of reducing a word to its stem through basic suffix stripping. For instance, the stem of the word “lively” is “live”, so running a the statement “She acted lively while helping me today.” through a stemmer (after tokenizing), would return the stems ‘act’, ‘live’, and ‘help’ with their respective words, while the rest of the words would just return the initial input (the words were already at their stem). \n",
    "\n",
    "\n",
    "Some common stemmers:\n",
    "\n",
    "* [Snowball](http://www.nltk.org/_modules/nltk/stem/snowball.html)\n",
    "\n",
    "* [Porter](http://www.nltk.org/_modules/nltk/stem/porter.html)\n",
    "\n",
    "* [Lancaster](http://www.nltk.org/_modules/nltk/stem/lancaster.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "da42cbda-6b5e-4854-9588-8a134e97b904"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stem of 'dogs' is 'dog'.\n",
      "The stem of 'creationism' is 'creation'.\n",
      "The stem of 'ate' is 'ate'.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.porter.PorterStemmer()\n",
    "words = ['dogs', 'creationism', 'ate']\n",
    "\n",
    "for word in words:\n",
    "    print \"The stem of '%s' is '%s'.\" %(word, porter.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1bf0f027-a1ed-4740-9ba4-8771035a5656"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization\n",
    "A problem that arises is that, for many cases, the stemmers will not return actual words. For instance, the stem of the words “excitement” and “excited” are both “excit”. Stemming is sufficient in many cases of interpreting the meaning of texts, but a better option in terms of meaning, is lemmatization. \n",
    "\n",
    "Lemmatization, on the other hand, is the reducing of a word to a common base word, known as the lemma. For instance, the word “excitement” reduces to the lemma “excite”, but also the words “are”, “am” and “is” reduce to the lemma “be”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemma of 'ate' is 'eat'.\n",
      "The lemma of 'dogs' is 'dog'.\n",
      "The lemma of 'creationism' is 'creationism'.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "taggedwords = {'dogs':'n', 'creationism':'n', 'ate':'v'}\n",
    "\n",
    "for word in taggedwords.keys():\n",
    "    print \"The lemma of '%s' is '%s'.\" %(word, wordnet_lemmatizer.lemmatize(word, taggedwords[word]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3686ebec-d738-49ea-8390-d99964a4a937"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Porter Algorithm](http://snowball.tartarus.org/algorithms/porter/stemmer.html)\n",
    "\n",
    "Porter's stemming algorithm consists of 5 phases of word reduction applied sequentially.\n",
    "\n",
    "For instance:\n",
    "\n",
    "$$SSES \\rightarrow SS \\\\ caress \\rightarrow care$$\n",
    "\n",
    "<br />\n",
    "$$S \\rightarrow \\\\ cats \\rightarrow cat$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78d40de8-591c-4664-8750-ab34138d9063"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Generator w/ Markov Chains\n",
    "\n",
    "Based off of [markovify python module](https://github.com/jsvine/markovify) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# %load ./mymarkovify/splitters.py\n",
    "'''\n",
    "import re\n",
    "\n",
    "ascii_lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "ascii_uppercase = ascii_lowercase.upper()\n",
    "\n",
    "# States w/ with thanks to https://github.com/unitedstates/python-us\n",
    "# Titles w/ thanks to https://github.com/nytimes/emphasis and @donohoe\n",
    "abbr_capped = \"|\".join([\n",
    "    \"ala|ariz|ark|calif|colo|conn|del|fla|ga|ill|ind|kan|ky|la|md|mass|mich|minn|miss|mo|mont|neb|nev|okla|ore|pa|tenn|vt|va|wash|wis|wyo\", # States\n",
    "    \"u.s\",\n",
    "    \"mr|ms|mrs|msr|dr|gov|pres|sen|sens|rep|reps|prof|gen|messrs|col|sr|jf|sgt|mgr|fr|rev|jr|snr|atty|supt\", # Titles\n",
    "    \"ave|blvd|st|rd|hwy\", # Streets\n",
    "    \"jan|feb|mar|apr|jun|jul|aug|sep|sept|oct|nov|dec\", # Months\n",
    "    \"|\".join(ascii_lowercase) # Initials\n",
    "]).split(\"|\")\n",
    "\n",
    "abbr_lowercase = \"etc|v|vs|viz|al|pct\"\n",
    "\n",
    "exceptions = \"U.S.|U.N.|E.U.|F.B.I.|C.I.A.\".split(\"|\")\n",
    "\n",
    "def is_abbreviation(dotted_word):\n",
    "    clipped = dotted_word[:-1]\n",
    "    if clipped[0] in ascii_uppercase:\n",
    "        if clipped.lower() in abbr_capped: return True\n",
    "        else: return False\n",
    "    else:\n",
    "        if clipped in abbr_lowercase: return True\n",
    "        else: return False\n",
    "\n",
    "def is_sentence_ender(word):\n",
    "    if word in exceptions: return False\n",
    "    if word[-1] in [ \"?\", \"!\" ]:\n",
    "        return True\n",
    "    if len(re.sub(r\"[^A-Z]\", \"\", word)) > 1:\n",
    "        return True\n",
    "    if word[-1] == \".\" and (not is_abbreviation(word)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    potential_end_pat = re.compile(r\"\".join([\n",
    "        r\"([\\w\\.'’&\\]\\)]+[\\.\\?!])\", # A word that ends with punctuation\n",
    "        r\"([‘’“”'\\\"\\)\\]]*)\", # Followed by optional quote/parens/etc\n",
    "        r\"(\\s+(?![a-z\\-–—]))\", # Followed by whitespace + non-(lowercase or dash)\n",
    "        ]), re.U)\n",
    "    dot_iter = re.finditer(potential_end_pat, text)\n",
    "    end_indices = [ (x.start() + len(x.group(1)) + len(x.group(2)))\n",
    "        for x in dot_iter\n",
    "        if is_sentence_ender(x.group(1)) ]\n",
    "    spans = zip([None] + end_indices, end_indices + [None])\n",
    "    sentences = [ text[start:end].strip() for start, end in spans ]\n",
    "    return sentences\n",
    "'''\n",
    "\n",
    "from nltk import tokenize\n",
    "sent_tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    return sent_tokenizer.tokenize(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2bdd5762-b642-43a7-b210-be5e7b3e462c"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "./ChopraEdited/Chopra-Deepak-Book-Of-Secrets_Clean.pdf\n",
      "./ChopraEdited/DeepakChopra_Interview_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-the_seven_spiritual_laws_of_yoga_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-superbrain_Clean.pdf\n",
      "./ChopraEdited/DeepakChopra_Quotes_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-the-7-laws-of-success_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-How-To-Know-God_Clean.pdf\n",
      "Done Reading.\n",
      "258456  words\n",
      "13050 sentences\n"
     ]
    }
   ],
   "source": [
    "import extract\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "#Extract text as a string\n",
    "cleantext = extract.read(directory = \"./ChopraEdited\")\n",
    "\n",
    "#Looking at the size of our data set\n",
    "stoken = nltk.tokenize.PunktSentenceTokenizer()\n",
    "wtoken = nltk.tokenize.WordPunctTokenizer()\n",
    "s = stoken.tokenize(cleantext)\n",
    "w = wtoken.tokenize(cleantext)\n",
    "print np.size(w), \" words\"\n",
    "print np.size(s), \"sentences\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "78df06a2-9c89-48bc-b8ec-0bc5afa56978"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mymarkovify\n",
    "\n",
    "#Train our model on the string\n",
    "text_model = mymarkovify.Text(cleantext, state_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "075afb3c-dd53-45c3-8d5d-73c29d67cefa"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Pay attention to your breath and your mind.\n",
      "\n",
      "2. With a magnifying glass you would see that the answer could be as simple as simply learning to be.\n",
      "\n",
      "3. My actions and thoughts are drawing on God's force field, as if we are to have any confidence in them.\n",
      "\n",
      "4. With the loss of unity.\n",
      "\n",
      "5. One can think of a photon as the archetype of all energy in the non local domain of our collective consciousness.\n",
      "\n",
      "6. Few people accept it, and those who want to grab the love and attention that used to be saffron-robed monks in meditation before dawn.\n",
      "\n",
      "7. Ten years ago, it would have made no difference to the matter.\n",
      "\n",
      "8. As it happens, I didn't have any awareness of being a co-creator.\n",
      "\n",
      "9. So do not think that when you succeed at doing this, you will be surprised how much easier it becomes in the future to flow into the present.\n",
      "\n",
      "10. Each of us is many people; you can define this in terms of what a person wants to achieve: 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print n generated sentences\n",
    "n = 10 #number of sentences to be generated\n",
    "for i in range(n):\n",
    "    print \"%s. %s\\n\" % (i+1, text_model.make_sentence(tries=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3bd3fdaf-fbbf-485a-b268-5c2918cc1125"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Classification\n",
    "\n",
    "Text Classification is the use of NLTK Classifiers to classify text into categories.\n",
    "\n",
    "\n",
    "NLTK Classifiers:\n",
    "\n",
    "\n",
    "* Naive Bayes Classifier\n",
    "\n",
    "\n",
    "* Maximum Entropy Classifier\n",
    "\n",
    "\n",
    "* Decision Tree\n",
    "\n",
    "\n",
    "* Scikit Learn Wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7e4ec686-f890-49ff-b3cb-0d45743d8e30"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Bayes Theorem : \n",
    "\n",
    "$$ P(C|x) = \\frac{P(x|C) P(C)}{P(x)} $$\n",
    "\n",
    "Naive Bayes Classification :\n",
    "\n",
    "Based on the assumption that all features ${x_1, x_2,......,x_i}$ are conditionally independent, given the category C\n",
    "\n",
    "or, $$ P(x_i|x_1, x_2,..,x_(i-1),x(i+1),...,C) = P(x_i|C) $$\n",
    "\n",
    "Then, $P(C|x_1,x_2,....,x_i)$ can be computed using Bayes Theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eb215dfb-5c3a-4245-9120-0e1aa66c41a1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Entropy Model\n",
    "\n",
    "The model considers probability distributions closest to empirical data and picks the one with highest entropy.\n",
    "\n",
    "### What does entropy mean?\n",
    "\n",
    "Entropy is measure of uncertainty or 'surprise' in the data. Assumption is that data is likely to have random, unknown elements.\n",
    "\n",
    "### What will have the highest entropy?\n",
    "\n",
    "Usually, uniform distribution will have the highest entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7c6c582e-7ee5-4fdc-8ef0-9839ffc562a7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of Text Classification - \n",
    "\n",
    "\n",
    "* Analyzing and classifying emails, texts and chats for security purposes\n",
    "\n",
    "\n",
    "* Data acquisition from social media posts for advertising\n",
    "\n",
    "\n",
    "* Analysis of speech patterns and transcripts for academic research (grammar development, language research, psychology, etc.)\n",
    "\n",
    "\n",
    "* Integrating NLP techniques with Artificial Intelligence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you!\n",
    "## Questions?\n",
    "\n",
    "![title](./Cubs.png)\n",
    "\n",
    "[Image source](http://webarebears.wikia.com/wiki/The_Bears)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
