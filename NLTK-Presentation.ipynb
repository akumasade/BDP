{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b05589f0-bc9f-4d3a-9b81-06ad5c99702b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python's Natural Language Toolkit - NLTK\n",
    " ##### Keziah Sheldon, Eesha Das Gupta, Rachel Buttry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "abbbf0bb-9f2d-443a-a519-8a6471d0795f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Natural Language Processing?\n",
    "\n",
    "\"Natural language processing is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages.\" -[Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "\n",
    "Link: [Wisdom of Chopra](http://wisdomofchopra.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c978d3f9-93c1-45ff-975b-ac796a1422ba"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLTK\n",
    "\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\" \n",
    "\n",
    "[Read More](http://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "62085755-f394-422d-a675-4e8cdf6b1469"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizing\n",
    "\n",
    "* Tokenization is the first step of NLP (Natural Language Processing)\n",
    "* Split strings into substrings\n",
    "* Separate into sentences, identifies where they start and stop\n",
    "\n",
    "\n",
    "Tokenize by sentence:\n",
    "Doesn’t just depend on punctuation since recent decades have much more ambiguous punctuation usage (emoticons, abbreviations etc.)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9169ad09-a6fc-4c66-bbe5-fef9e66c7859"
    }
   },
   "source": [
    ">>> text = “this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.”\n",
    ">>> from nltk.tokenize import sent_tokenize\n",
    ">>> sent_tokenize_list = sent_tokenize(text)\n",
    ">>> sent_tokenize_list\n",
    "[“this’s a sent tokenize test.”, ‘this is sent two.’, ‘is this sent three?’, ‘sent 4 is cool!’, “Now it’s your turn.”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fabb1615-376b-4b18-90e4-bfdb4fce8bda"
    }
   },
   "source": [
    "Tokenize by word: (simpler than sentence tokenizer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "0d428832-0e0e-401f-8744-db1b9fa2d03d"
    }
   },
   "source": [
    ">>> from nltk.tokenize import word_tokenize\n",
    ">>> word_tokenize(‘Hello World.’)\n",
    "[‘Hello’, ‘World’, ‘.’]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78cd1ee7-a50f-412e-adf5-6035341a4c69"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "\n",
    "* POS is grammatical tagging\n",
    "* Separates into nouns, adjectives, verbs etc.\n",
    "* Can use pre-trained taggers or train yourself\n",
    "\n",
    "Types of POS tags:\n",
    "* JJ: adjective or numeral, ordinal\n",
    "* NNP: noun, proper, singular\n",
    "* IN: preposition or conjunction, subordinating\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7fab5b42-dabe-48c8-8557-67edd7755575"
    }
   },
   "source": [
    ">>> text = nltk.word_tokenize(“Dive into NLTK: Part-of-speech tagging and POS Tagger”)\n",
    ">>> nltk.pos_tag(text)\n",
    "[(‘Dive’, ‘JJ’), (‘into’, ‘IN’), (‘NLTK’, ‘NNP’), (‘:’, ‘:’), (‘Part-of-speech’, ‘JJ’), (‘tagging’, ‘NN’), (‘and’, ‘CC’), (‘POS’, ‘NNP’), (‘Tagger’, ‘NNP’)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92f780f0-b790-4b26-87bb-d934e3c7ef82"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the the process of reducing a word to its stem through basic suffix stripping. For instance, the stem of the word “lively” is “live”, so running a the statement “She acted lively while helping me today.” through a stemmer (after tokenizing), would return the stems ‘act’, ‘live’, and ‘help’ with their respective words, while the rest of the words would just return the initial input (the words were already at their stem). \n",
    "\n",
    "\n",
    "Some common stemmers:\n",
    "\n",
    "* Snowball : http://www.nltk.org/_modules/nltk/stem/snowball.html\n",
    "\n",
    "* Porter : http://www.nltk.org/_modules/nltk/stem/porter.html\n",
    "\n",
    "* Lancaster : http://www.nltk.org/_modules/nltk/stem/lancaster.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "da42cbda-6b5e-4854-9588-8a134e97b904"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stem of 'expectations' is 'expect'.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "word = 'expectations'\n",
    "print \"The stem of '%s' is '%s'.\" %(word, snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3686ebec-d738-49ea-8390-d99964a4a937"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Porter Algorithm\n",
    "\n",
    "Porter's stemming algorithm consists of 5 phases of word reduction applied sequentially.\n",
    "\n",
    "For instance:\n",
    "\n",
    "$$SSES \\rightarrow SS \\\\ caress \\rightarrow care$$\n",
    "\n",
    "<br />\n",
    "$$S \\rightarrow \\\\ cats \\rightarrow cat$$\n",
    "\n",
    "[Read More](http://snowball.tartarus.org/algorithms/porter/stemmer.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0ae3e29d-fe40-411c-ab42-5a2b09d79df4"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A problem that arises is that, for many cases, the stemmers will not return actual words. For instance, the stem of the words “excitement” and “excited” are both “excit”. Stemming is sufficient in many cases of interpreting the meaning of texts, but a better option in terms of meaning, is lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1bf0f027-a1ed-4740-9ba4-8771035a5656"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization\n",
    "Lemmatization, on the other hand, is the reducing of a word to a common base word, known as the lemma. For instance, the word “excitement” reduces to the lemma “excite”, but also the words “are”, “am” and “is” reduce to the lemma “be”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78d40de8-591c-4664-8750-ab34138d9063"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Generator w/ Markov Chains\n",
    "\n",
    "Based off of [markovify python module](https://github.com/jsvine/markovify) \n",
    "\n",
    "(pip install markovify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# %load ./mymarkovify/splitters.py\n",
    "import re\n",
    "from nltk import tokenize\n",
    "\n",
    "'''\n",
    "ascii_lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "ascii_uppercase = ascii_lowercase.upper()\n",
    "sent_tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "\n",
    "# States w/ with thanks to https://github.com/unitedstates/python-us\n",
    "# Titles w/ thanks to https://github.com/nytimes/emphasis and @donohoe\n",
    "abbr_capped = \"|\".join([\n",
    "    \"ala|ariz|ark|calif|colo|conn|del|fla|ga|ill|ind|kan|ky|la|md|mass|mich|minn|miss|mo|mont|neb|nev|okla|ore|pa|tenn|vt|va|wash|wis|wyo\", # States\n",
    "    \"u.s\",\n",
    "    \"mr|ms|mrs|msr|dr|gov|pres|sen|sens|rep|reps|prof|gen|messrs|col|sr|jf|sgt|mgr|fr|rev|jr|snr|atty|supt\", # Titles\n",
    "    \"ave|blvd|st|rd|hwy\", # Streets\n",
    "    \"jan|feb|mar|apr|jun|jul|aug|sep|sept|oct|nov|dec\", # Months\n",
    "    \"|\".join(ascii_lowercase) # Initials\n",
    "]).split(\"|\")\n",
    "\n",
    "abbr_lowercase = \"etc|v|vs|viz|al|pct\"\n",
    "\n",
    "exceptions = \"U.S.|U.N.|E.U.|F.B.I.|C.I.A.\".split(\"|\")\n",
    "\n",
    "def is_abbreviation(dotted_word):\n",
    "    clipped = dotted_word[:-1]\n",
    "    if clipped[0] in ascii_uppercase:\n",
    "        if clipped.lower() in abbr_capped: return True\n",
    "        else: return False\n",
    "    else:\n",
    "        if clipped in abbr_lowercase: return True\n",
    "        else: return False\n",
    "\n",
    "def is_sentence_ender(word):\n",
    "    if word in exceptions: return False\n",
    "    if word[-1] in [ \"?\", \"!\" ]:\n",
    "        return True\n",
    "    if len(re.sub(r\"[^A-Z]\", \"\", word)) > 1:\n",
    "        return True\n",
    "    if word[-1] == \".\" and (not is_abbreviation(word)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    potential_end_pat = re.compile(r\"\".join([\n",
    "        r\"([\\w\\.'’&\\]\\)]+[\\.\\?!])\", # A word that ends with punctuation\n",
    "        r\"([‘’“”'\\\"\\)\\]]*)\", # Followed by optional quote/parens/etc\n",
    "        r\"(\\s+(?![a-z\\-–—]))\", # Followed by whitespace + non-(lowercase or dash)\n",
    "        ]), re.U)\n",
    "    dot_iter = re.finditer(potential_end_pat, text)\n",
    "    end_indices = [ (x.start() + len(x.group(1)) + len(x.group(2)))\n",
    "        for x in dot_iter\n",
    "        if is_sentence_ender(x.group(1)) ]\n",
    "    spans = zip([None] + end_indices, end_indices + [None])\n",
    "    sentences = [ text[start:end].strip() for start, end in spans ]\n",
    "    return sentences\n",
    "'''\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    return sent_tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2bdd5762-b642-43a7-b210-be5e7b3e462c"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "./ChopraEdited/DeepakChopra_Interview_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-the_seven_spiritual_laws_of_yoga_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-How-To-Know-God_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-Book-Of-Secrets_Clean.pdf\n",
      "./ChopraEdited/DeepakChopra_Quotes_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-superbrain_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-the-7-laws-of-success_Clean.pdf\n",
      "Done Reading.\n",
      "258477 13051\n"
     ]
    }
   ],
   "source": [
    "import extract\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "cleantext = extract.read(directory = \"./ChopraEdited\")\n",
    "\n",
    "stoken = nltk.tokenize.PunktSentenceTokenizer()\n",
    "wtoken = nltk.tokenize.WordPunctTokenizer()\n",
    "s = stoken.tokenize(cleantext)\n",
    "w = wtoken.tokenize(cleantext)\n",
    "print np.size(w)\n",
    "print np.size(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "78df06a2-9c89-48bc-b8ec-0bc5afa56978"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mymarkovify\n",
    "\n",
    "#Train our model on the string\n",
    "text_model = mymarkovify.Text(cleantext, state_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "075afb3c-dd53-45c3-8d5d-73c29d67cefa"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Can we truly satisfy the demands of our egos, who want to grab the love and attention that used to be seen by the eyes as photons, it doesn't suddenly jump into material existence.\n",
      "\n",
      "2. Sometimes the connections are faultI might have come up with the fewest delays, obstacles, and backslidinadopting the right belief is much more elusive and even mystical.\n",
      "\n",
      "3. Now slowly lower both legs to the floor.\n",
      "\n",
      "4. It is inevitable that you will escape from evil.\n",
      "\n",
      "5. Many of India's saints strike me with less than wonder, and I have a separate stake in the world, you are aligned with your creative juices, the expressions that emerge arise effortlessly.\n",
      "\n",
      "6. To a co-creator, life has a tendency to show up in the word It.\n",
      "\n",
      "7. To reach this state of innocence would be impossible if he didn't want to be known.\n",
      "\n",
      "8. A stranger makes you feel like a choice anymore.\n",
      "\n",
      "9. Karma is just experiences that make for a healing relationship is that life is a miracle; God works through me, my greatest joy is service to him.\n",
      "\n",
      "10. At her age Tars life is all ups and downs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print n generated sentences\n",
    "n = 10 #number of sentences to be generated\n",
    "for i in range(n):\n",
    "    print \"%s. %s\\n\" % (i+1, text_model.make_sentence(tries=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3bd3fdaf-fbbf-485a-b268-5c2918cc1125"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Classification\n",
    "\n",
    "Text Classification is the use of NLTK Classifiers to classify text into categories.\n",
    "\n",
    "\n",
    "NLTK Classifiers:\n",
    "\n",
    "\n",
    "* Naive Bayes Classifier\n",
    "\n",
    "\n",
    "* Maximum Entropy Classifier\n",
    "\n",
    "\n",
    "* Decision Tree\n",
    "\n",
    "\n",
    "* Scikit Learn Wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7e4ec686-f890-49ff-b3cb-0d45743d8e30"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Bayes Theorem : \n",
    "\n",
    "$$ P(C|x) = \\frac{P(x|C) P(C)}{P(x)} $$\n",
    "\n",
    "Naive Bayes Classification :\n",
    "\n",
    "Based on the assumption that all features ${x_1, x_2,......,x_i}$ are conditionally independent, given the category C\n",
    "\n",
    "or, $$ P(x_i|x_1, x_2,..,x_(i-1),x(i+1),..., x_i,C) = P(x_i|C) $$\n",
    "\n",
    "Then, $P(C|x_1,x_2,....,x_i)$ can be computed using Bayes Theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eb215dfb-5c3a-4245-9120-0e1aa66c41a1"
    }
   },
   "source": [
    "## Maximum Entropy Model\n",
    "\n",
    "The model considers probability distributions closest to empirical data and picks the one with highest entropy.\n",
    "\n",
    "### What does entropy mean?\n",
    "\n",
    "Entropy is measure of uncertainty or 'surprise' in the data. Assumption is that data is likely to have random, unknown elements.\n",
    "\n",
    "### What will have the highest entropy?\n",
    "\n",
    "Usually, uniform distribution will have the highest entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7c6c582e-7ee5-4fdc-8ef0-9839ffc562a7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of Text Classification - \n",
    "\n",
    "\n",
    "* Analyzing and classifying emails, texts and chats for security purposes\n",
    "\n",
    "\n",
    "* Data acquisition from social media posts for advertising\n",
    "\n",
    "\n",
    "* Analysis of speech patterns and transcripts for academic research (grammar development, language research, psychology, etc.)\n",
    "\n",
    "\n",
    "* Integrating NLP techniques with Artificial Intelligence\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
