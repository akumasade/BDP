{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b05589f0-bc9f-4d3a-9b81-06ad5c99702b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python's Natural Language Toolkit - NLTK\n",
    " ##### Keziah Sheldon, Eesha Das Gupta, Rachel Buttry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "abbbf0bb-9f2d-443a-a519-8a6471d0795f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Natural Language Processing?\n",
    "\n",
    "\"Natural language processing is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages.\" -[Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "\n",
    "Natural Language Processing is the way for computers to process the human spoken and written language for analysis.\n",
    "\n",
    "Link: [Wisdom of Chopra](http://wisdomofchopra.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c978d3f9-93c1-45ff-975b-ac796a1422ba"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Natural Language ToolKit](http://www.nltk.org/)\n",
    "\n",
    "* Python module for NLP\n",
    "* Provides interfaces to over 50 corpora and lexical resources\n",
    "* Tools for tokenization, stemming, tagging, parsing, andd semantic reasoning\n",
    "* Wrappers for industrial-strength NLP libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Example NLP Pipeline for NLTK\n",
    "\n",
    "![title](./pipeline1.png)\n",
    "\n",
    "[Image source](http://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "62085755-f394-422d-a675-4e8cdf6b1469"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizing\n",
    "\n",
    "* Tokenization is the first step of NLP (Natural Language Processing)\n",
    "* Split strings into substrings\n",
    "* Separate into sentences, identifies where they start and stop\n",
    "\n",
    "### For example, the Punkt tokenizer: (from docstring)\n",
    "This tokenizer divides a text into a list of sentences,\n",
    "by using an unsupervised algorithm to build a model for abbreviation\n",
    "words, collocations, and words that start sentences.  It must be\n",
    "trained on a large collection of plaintext in the target language\n",
    "before it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenize by sentence:\n",
    "* Doesn’t just depend on punctuation since recent decades have much more ambiguous punctuation usage (emoticons, abbreviations etc.)\n",
    "* Depends on the pre-trained 'tokenizer', e.g, Punkt, Stanford, Regex etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is PHYST480 Big Data class.', 'Natural language processing with the Natural Language Tool Kit module.', 'Much machine learning.', 'So wow.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"This is PHYST480 Big Data class. Natural language processing with the Natural Language Tool Kit module. Much machine learning. So wow.\"\n",
    "\n",
    "sent_tokenize_list = nltk.tokenize.sent_tokenize(text)\n",
    "\n",
    "print sent_tokenize_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fabb1615-376b-4b18-90e4-bfdb4fce8bda"
    }
   },
   "source": [
    "## Tokenize by word\n",
    "* Splits string by word\n",
    "* Punkt tokenizer also has word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.']\n"
     ]
    }
   ],
   "source": [
    "print nltk.tokenize.word_tokenize('Hello World.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78cd1ee7-a50f-412e-adf5-6035341a4c69"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "\n",
    "* POS is grammatical tagging\n",
    "* Separates into nouns, adjectives, verbs etc.\n",
    "* Can use pre-trained taggers or train yourself\n",
    "\n",
    "Types of POS tags:\n",
    "* JJ: adjective or numeral, ordinal\n",
    "* NNP: noun, proper, singular\n",
    "* IN: preposition or conjunction, subordinating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('cake', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('lie', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = nltk.word_tokenize('The cake is a lie.')\n",
    "\n",
    "print nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92f780f0-b790-4b26-87bb-d934e3c7ef82"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the the process of reducing a word to its stem through basic suffix stripping. For instance, the stem of the word “lively” is “live”, so running a the statement “She acted lively while helping me today.” through a stemmer (after tokenizing), would return the stems ‘act’, ‘live’, and ‘help’ with their respective words, while the rest of the words would just return the initial input (the words were already at their stem). \n",
    "\n",
    "\n",
    "Some common stemmers:\n",
    "\n",
    "* [Snowball](http://www.nltk.org/_modules/nltk/stem/snowball.html)\n",
    "\n",
    "* [Porter](http://www.nltk.org/_modules/nltk/stem/porter.html)\n",
    "\n",
    "* [Lancaster](http://www.nltk.org/_modules/nltk/stem/lancaster.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "da42cbda-6b5e-4854-9588-8a134e97b904"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stem of 'dogs' is 'dog'.\n",
      "The stem of 'creationism' is 'creation'.\n",
      "The stem of 'ate' is 'ate'.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.porter.PorterStemmer()\n",
    "words = ['dogs', 'creationism', 'ate']\n",
    "\n",
    "for word in words:\n",
    "    print \"The stem of '%s' is '%s'.\" %(word, porter.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3686ebec-d738-49ea-8390-d99964a4a937"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Porter Algorithm](http://snowball.tartarus.org/algorithms/porter/stemmer.html)\n",
    "\n",
    "Porter's stemming algorithm consists of 5 phases of word reduction applied sequentially.\n",
    "\n",
    "For instance:\n",
    "\n",
    "$$SSES \\rightarrow SS \\\\ caress \\rightarrow care$$\n",
    "\n",
    "<br />\n",
    "$$S \\rightarrow \\\\ cats \\rightarrow cat$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1bf0f027-a1ed-4740-9ba4-8771035a5656"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization\n",
    "A problem that arises is that, for many cases, the stemmers will not return actual words. For instance, the stem of the words “excitement” and “excited” are both “excit”. Stemming is sufficient in many cases of interpreting the meaning of texts, but a better option in terms of meaning, is lemmatization. \n",
    "\n",
    "Lemmatization, on the other hand, is the reducing of a word to a common base word, known as the lemma. For instance, the word “excitement” reduces to the lemma “excite”, but also the words “are”, “am” and “is” reduce to the lemma “be”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemma of 'ate' is 'eat'.\n",
      "The lemma of 'dogs' is 'dog'.\n",
      "The lemma of 'creationism' is 'creationism'.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "taggedwords = {'dogs':'n', 'creationism':'n', 'ate':'v'}\n",
    "\n",
    "for word in taggedwords.keys():\n",
    "    print \"The lemma of '%s' is '%s'.\" %(word, wordnet_lemmatizer.lemmatize(word, taggedwords[word]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "78d40de8-591c-4664-8750-ab34138d9063"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Generator w/ Markov Chains\n",
    "\n",
    "Based off of [markovify python module](https://github.com/jsvine/markovify) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# %load ./mymarkovify/splitters.py\n",
    "'''\n",
    "import re\n",
    "\n",
    "ascii_lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "ascii_uppercase = ascii_lowercase.upper()\n",
    "\n",
    "# States w/ with thanks to https://github.com/unitedstates/python-us\n",
    "# Titles w/ thanks to https://github.com/nytimes/emphasis and @donohoe\n",
    "abbr_capped = \"|\".join([\n",
    "    \"ala|ariz|ark|calif|colo|conn|del|fla|ga|ill|ind|kan|ky|la|md|mass|mich|minn|miss|mo|mont|neb|nev|okla|ore|pa|tenn|vt|va|wash|wis|wyo\", # States\n",
    "    \"u.s\",\n",
    "    \"mr|ms|mrs|msr|dr|gov|pres|sen|sens|rep|reps|prof|gen|messrs|col|sr|jf|sgt|mgr|fr|rev|jr|snr|atty|supt\", # Titles\n",
    "    \"ave|blvd|st|rd|hwy\", # Streets\n",
    "    \"jan|feb|mar|apr|jun|jul|aug|sep|sept|oct|nov|dec\", # Months\n",
    "    \"|\".join(ascii_lowercase) # Initials\n",
    "]).split(\"|\")\n",
    "\n",
    "abbr_lowercase = \"etc|v|vs|viz|al|pct\"\n",
    "\n",
    "exceptions = \"U.S.|U.N.|E.U.|F.B.I.|C.I.A.\".split(\"|\")\n",
    "\n",
    "def is_abbreviation(dotted_word):\n",
    "    clipped = dotted_word[:-1]\n",
    "    if clipped[0] in ascii_uppercase:\n",
    "        if clipped.lower() in abbr_capped: return True\n",
    "        else: return False\n",
    "    else:\n",
    "        if clipped in abbr_lowercase: return True\n",
    "        else: return False\n",
    "\n",
    "def is_sentence_ender(word):\n",
    "    if word in exceptions: return False\n",
    "    if word[-1] in [ \"?\", \"!\" ]:\n",
    "        return True\n",
    "    if len(re.sub(r\"[^A-Z]\", \"\", word)) > 1:\n",
    "        return True\n",
    "    if word[-1] == \".\" and (not is_abbreviation(word)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    potential_end_pat = re.compile(r\"\".join([\n",
    "        r\"([\\w\\.'’&\\]\\)]+[\\.\\?!])\", # A word that ends with punctuation\n",
    "        r\"([‘’“”'\\\"\\)\\]]*)\", # Followed by optional quote/parens/etc\n",
    "        r\"(\\s+(?![a-z\\-–—]))\", # Followed by whitespace + non-(lowercase or dash)\n",
    "        ]), re.U)\n",
    "    dot_iter = re.finditer(potential_end_pat, text)\n",
    "    end_indices = [ (x.start() + len(x.group(1)) + len(x.group(2)))\n",
    "        for x in dot_iter\n",
    "        if is_sentence_ender(x.group(1)) ]\n",
    "    spans = zip([None] + end_indices, end_indices + [None])\n",
    "    sentences = [ text[start:end].strip() for start, end in spans ]\n",
    "    return sentences\n",
    "'''\n",
    "\n",
    "from nltk import tokenize\n",
    "sent_tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    return sent_tokenizer.tokenize(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2bdd5762-b642-43a7-b210-be5e7b3e462c"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "./ChopraEdited/DeepakChopra_Interview_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-the_seven_spiritual_laws_of_yoga_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-How-To-Know-God_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-Book-Of-Secrets_Clean.pdf\n",
      "./ChopraEdited/DeepakChopra_Quotes_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-superbrain_Clean.pdf\n",
      "./ChopraEdited/Chopra-Deepak-the-7-laws-of-success_Clean.pdf\n",
      "Done Reading.\n",
      "258477  words\n",
      "13051 sentences\n"
     ]
    }
   ],
   "source": [
    "import extract\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "#Extract text as a string\n",
    "cleantext = extract.read(directory = \"./ChopraEdited\")\n",
    "\n",
    "#Looking at the size of our data set\n",
    "stoken = nltk.tokenize.PunktSentenceTokenizer()\n",
    "wtoken = nltk.tokenize.WordPunctTokenizer()\n",
    "s = stoken.tokenize(cleantext)\n",
    "w = wtoken.tokenize(cleantext)\n",
    "print np.size(w), \" words\"\n",
    "print np.size(s), \"sentences\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "78df06a2-9c89-48bc-b8ec-0bc5afa56978"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mymarkovify\n",
    "\n",
    "#Train our model on the string\n",
    "text_model = mymarkovify.Text(cleantext, state_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "075afb3c-dd53-45c3-8d5d-73c29d67cefa"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. In a state of being in the movie, outside the move, and the movie itself.\n",
      "\n",
      "2. As it happens, I didn't have any awareness of it.\n",
      "\n",
      "3. Nothing gets past us, no matter how obviously they deserve it, remind yourself that self-acceptance is the source of all that exists is not an option.\n",
      "\n",
      "4. You can create anything because you are in stage five can make every wish come true, the ones that should come true matter more.\n",
      "\n",
      "5. Although this cycle expresses itself in many ways, but this one says something about our stage one God.\n",
      "\n",
      "6. If you perform asanas regularly, you will feel more guilty, adding to the burden of failure.\n",
      "\n",
      "7. Next you will likely experience the return of worries, whatever is hanging over your head and leg to theoor while exhaling.\n",
      "\n",
      "8. Recognizing this, Shankara named the physical body and mind come together.\n",
      "\n",
      "9. We have outgrown the need for creative expression and renewal.\n",
      "\n",
      "10. By going inside yourself, you can access wisdom and knowledge about anything in creation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print n generated sentences\n",
    "n = 10 #number of sentences to be generated\n",
    "for i in range(n):\n",
    "    print \"%s. %s\\n\" % (i+1, text_model.make_sentence(tries=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3bd3fdaf-fbbf-485a-b268-5c2918cc1125"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Classification\n",
    "\n",
    "Text Classification is the use of NLTK Classifiers to classify text into categories.\n",
    "\n",
    "\n",
    "NLTK Classifiers:\n",
    "\n",
    "\n",
    "* Naive Bayes Classifier\n",
    "\n",
    "\n",
    "* Maximum Entropy Classifier\n",
    "\n",
    "\n",
    "* Decision Tree\n",
    "\n",
    "\n",
    "* Scikit Learn Wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7e4ec686-f890-49ff-b3cb-0d45743d8e30"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Bayes Theorem : \n",
    "\n",
    "$$ P(C|x) = \\frac{P(x|C) P(C)}{P(x)} $$\n",
    "\n",
    "Naive Bayes Classification :\n",
    "\n",
    "Based on the assumption that all features ${x_1, x_2,......,x_i}$ are conditionally independent, given the category C\n",
    "\n",
    "or, $$ P(x_i|x_1, x_2,..,x_(i-1),x(i+1),..., x_i,C) = P(x_i|C) $$\n",
    "\n",
    "Then, $P(C|x_1,x_2,....,x_i)$ can be computed using Bayes Theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eb215dfb-5c3a-4245-9120-0e1aa66c41a1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Entropy Model\n",
    "\n",
    "The model considers probability distributions closest to empirical data and picks the one with highest entropy.\n",
    "\n",
    "### What does entropy mean?\n",
    "\n",
    "Entropy is measure of uncertainty or 'surprise' in the data. Assumption is that data is likely to have random, unknown elements.\n",
    "\n",
    "### What will have the highest entropy?\n",
    "\n",
    "Usually, uniform distribution will have the highest entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7c6c582e-7ee5-4fdc-8ef0-9839ffc562a7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of Text Classification - \n",
    "\n",
    "\n",
    "* Analyzing and classifying emails, texts and chats for security purposes\n",
    "\n",
    "\n",
    "* Data acquisition from social media posts for advertising\n",
    "\n",
    "\n",
    "* Analysis of speech patterns and transcripts for academic research (grammar development, language research, psychology, etc.)\n",
    "\n",
    "\n",
    "* Integrating NLP techniques with Artificial Intelligence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "## Questions?\n",
    "\n",
    "![title](./Cubs.png)\n",
    "\n",
    "[Image source](http://webarebears.wikia.com/wiki/The_Bears)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
