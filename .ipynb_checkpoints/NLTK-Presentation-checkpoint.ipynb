{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python's Natural Language Toolkit - NLTK\n",
    " ##### Keziah Sheldon, Eesha Das Gupta, Rachel Buttry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\"Natural language processing is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human–computer interaction.\"-Wikipedia (remember to put in hyperlink:https://en.wikipedia.org/wiki/Natural_language_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-b98e7091aabe>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-b98e7091aabe>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    tweets = nltk.corpus.twitter_samples.\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re \n",
    "tweets = nltk.corpus.twitter_samples. \n",
    "#]print ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the the process of reducing a word to its stem through basic suffix stripping. For instance, the stem of the word “lively” is “live”, so running a the statement “She acted lively while helping me today.” through a stemmer (after tokenizing), would return the stems ‘act’, ‘live’, and ‘help’ with their respective words, while the rest of the words would just return the initial input (the words were already at their stem). \n",
    "\n",
    "\n",
    "Some common stemmers:\n",
    "\n",
    "Snowball : http://www.nltk.org/_modules/nltk/stem/snowball.html\n",
    "\n",
    "Porter : http://www.nltk.org/_modules/nltk/stem/porter.html\n",
    "\n",
    "Lancaster : http://www.nltk.org/_modules/nltk/stem/lancaster.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stem of 'expectations' is 'expect'.\n"
     ]
    }
   ],
   "source": [
    "snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "word = 'expectations'\n",
    "print r\"The stem of '{}' is '{}'.\".format(word, snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem that arises is that, for many cases, the stemmers will not return actual words. For instance, the stem of the words “excitement” and “excited” are both “excit”. Stemming is sufficient in many cases of interpreting the meaning of texts, but a better option in terms of meaning, is lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "Lemmatization, on the other hand, is the reducing of a word to a common base word, known as the lemma. For instance, the word “excitement” reduces to the lemma “excite”, but also the words “are”, “am” and “is” reduce to the lemma “be”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "Text Classification is the use of NLTK Classifiers to classify text into categories.\n",
    "\n",
    "\n",
    "NLTK Classifiers:\n",
    "\n",
    "\n",
    "* classifier.naivebayes module based on the Naive Bayes assumption\n",
    "\n",
    "* classifier.maxent module implements Generalized Iterative Scaling and Improved Iterative Scaling to define a maximum entropy model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Text Classification - \n",
    "\n",
    "\n",
    "* Analyzing and classifying emails, texts and chats for security purposes\n",
    "\n",
    "\n",
    "* Data acquisition from social media posts for advertising\n",
    "\n",
    "\n",
    "* Analysis of speech patterns and transcripts for academic research (grammar development, language research, psychology, etc.)\n",
    "\n",
    "\n",
    "* Integrating NLP techniques with Artificial Intelligence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
